---
title: "Efficient fine-tuning of small-parameter large language models for biomedical bilingual multi-task applications"
collection: publications
category: 'journal-articles'
permalink: /publication/2025-05-15-Efficient-fine-tuning-LLM
excerpt: 'This study proposes an efficient fine-tuning framework for small-parameter large language models (LLMs) to handle biomedical bilingual multi-task applications, balancing performance with computational efficiency.'
date: 2025-05-15
venue: 'Applied Soft Computing'
paperurl: 'https://doi.org/10.1016/j.asoc.2025.113084'
citation: 'Li, Y., Yan, Y., Tong, Z., Wang, Y., Yang, Y., Bai, M., Pu, D., Xie, J., Liu, C., Li, B., Liu, M., & Shu, K. (2025). &quot;Efficient fine-tuning of small-parameter large language models for biomedical bilingual multi-task applications.&quot; <i>Applied Soft Computing</i>, 175, 113084.'
---

### Abstract

The escalating computational costs of large language models (LLMs) have catalyzed the pursuit of more efficient alternatives, particularly in specialized domains like biomedicine.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEcfrNvI5xw284raGT11L-fCN7VPVayWg2o7JmEkHAcL5y12ERMfgtD_CdhEbzhLfJG2Q59YT80wOb0yIMk4RGi8w5RWxogl_RxYt9d9OxH1Hc2y982BW30FlKJyFOjb_lyuVwlU7B9xw%3D%3D)] This study introduces a novel approach for fine-tuning small-parameter LLMs to address biomedical bilingual (Chinese-English) multi-task challenges. By employing an efficient fine-tuning strategy, we demonstrate that small-parameter models can achieve competitive performance comparable to larger counterparts while significantly reducing resource consumption.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEcfrNvI5xw284raGT11L-fCN7VPVayWg2o7JmEkHAcL5y12ERMfgtD_CdhEbzhLfJG2Q59YT80wOb0yIMk4RGi8w5RWxogl_RxYt9d9OxH1Hc2y982BW30FlKJyFOjb_lyuVwlU7B9xw%3D%3D)] The proposed method effectively captures domain-specific knowledge and enhances generalization capabilities across various biomedical tasks, offering a viable solution for resource-constrained environments.

### Highlights

*   Proposed an efficient fine-tuning framework tailored for small-parameter LLMs in the biomedical domain.
*   Achieved superior performance in bilingual (Chinese-English) multi-task scenarios, validating the model's robustness.
*   Demonstrated that strategically fine-tuned small models can rival larger LLMs, significantly lowering deployment costs and latency.

### Publication Details

*   **DOI:** [10.1016/j.asoc.2025.113084](https://doi.org/10.1016/j.asoc.2025.113084)
*   **Journal Impact Factor (IF):** 6.6
*   **CAS Ranking:** 中科院二区 Top
*   **Authors:** Yinghong Li*#, Yudong Yan#, Zhuohao Tong, Yu Wang, Yinqi Yang, Mingze Bai, Dan Pu, Jiazheng Xie, Chuan Liu, Bo Li*, Mingwei Liu, Kunxian Shu*